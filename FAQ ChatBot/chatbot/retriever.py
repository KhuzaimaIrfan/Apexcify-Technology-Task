# chatbot/retriever.py
import json
import os
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import joblib
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from chatbot.data_loader import load_site_chunks_jsonl
from chatbot.preprocess import preprocess_text


@dataclass
class RetrievalResult:
    score: float
    source: str
    chunk_id: int
    text: str
    title: str = ""




class WebsiteTfidfRetriever:
    """
    Answers ONLY from crawled website content.
    Data source: data/site_chunks.jsonl (generated by your crawler/chunker scripts)
    Index stored in: models/
    """

    def __init__(self, model_dir: str = "models", chunks_path: str = "data/site_chunks.jsonl"):
        self.model_dir = model_dir
        self.chunks_path = chunks_path

        self.vectorizer: Optional[TfidfVectorizer] = None
        self.matrix = None
        self.documents: List[Dict[str, Any]] = []

        self.vectorizer_path = os.path.join(self.model_dir, "tfidf_vectorizer.joblib")
        self.matrix_path = os.path.join(self.model_dir, "tfidf_matrix.joblib")
        self.docs_path = os.path.join(self.model_dir, "documents.jsonl")

    def index_exists(self) -> bool:
        return (
            os.path.exists(self.vectorizer_path)
            and os.path.exists(self.matrix_path)
            and os.path.exists(self.docs_path)
        )

    def build_index(self):
        os.makedirs(self.model_dir, exist_ok=True)

        docs = load_site_chunks_jsonl(self.chunks_path)
        # Boost: text + title (repeated to ensure weight)
        corpus = [preprocess_text(d["title"] + " " + d["title"] + " " + d["text"]) for d in docs]

        self.vectorizer = TfidfVectorizer(
            ngram_range=(1, 2),
            use_idf=False,      # Small corpus: rely on term frequency only
            sublinear_tf=True,  # Log scaling (1 + log(tf)) to dampen repetitions
            norm='l2',          # Cosine similarity normalization
            lowercase=False,    # already preprocessed
            strip_accents="unicode",
        )
        self.matrix = self.vectorizer.fit_transform(corpus)
        self.documents = docs

        self._save_model()

    def _save_model(self):
        joblib.dump(self.vectorizer, self.vectorizer_path)
        joblib.dump(self.matrix, self.matrix_path)
        with open(self.docs_path, "w", encoding="utf-8") as f:
            for d in self.documents:
                f.write(json.dumps(d, ensure_ascii=False) + "\n")

    def load(self):
        if not self.index_exists():
            # build automatically on first run
            self.build_index()

        self.vectorizer = joblib.load(self.vectorizer_path)
        self.matrix = joblib.load(self.matrix_path)
        
        self.documents = []
        with open(self.docs_path, "r", encoding="utf-8") as f:
            for line in f:
                self.documents.append(json.loads(line))
        return self

    def query(self, user_query: str, threshold: float = 0.05, top_k: int = 5) -> List[RetrievalResult]:
        if self.vectorizer is None or self.matrix is None:
            raise RuntimeError("Retriever not loaded. Call .load() first.")

        q = preprocess_text(user_query)
        q_vec = self.vectorizer.transform([q])
        
        # Calculate similarities
        sims = cosine_similarity(q_vec, self.matrix).flatten()
        
        # Get top indices
        top_indices = sims.argsort()[::-1][:top_k]
        
        results = []
        for i in top_indices:
            score = float(sims[i])
            if score < threshold:
                continue

            doc = self.documents[int(i)]
            
            # Return single chunk for conciseness as requested
            text_window = doc["text"]

            results.append(
                RetrievalResult(
                    score=score,
                    source=doc.get("source", ""),
                    chunk_id=int(doc.get("chunk_id", 0)),
                    text=text_window,
                    title=doc.get("title", ""),
                )
            )
        return results

